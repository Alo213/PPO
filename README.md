# PPO
Este repositorio es una implementaci칩n funcional lo m치s simple posible del algoritmo [Proximal Policy Optimization](https://arxiv.org/abs/1707.06347) con fines educativos. Esta implementaci칩n fue hecha tomando como base el [blog](https://medium.com/analytics-vidhya/coding-ppo-from-scratch-with-pytorch-part-1-4-613dfc1b14c8) de Eric Yang Yu @ UC San Diego.

### Pseudoc칩digo:

<img src=PPO-Clip-Pseudocode.png align="center"/>

