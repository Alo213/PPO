# PPO
Este repositorio es una implementaci칩n funcional lo m치s simple posible del algoritmo [Proximal Policy Optimization](https://arxiv.org/abs/1707.06347) con fines educativos. Esta implementaci칩n fue hecha tomando como base el [blog](https://medium.com/analytics-vidhya/coding-ppo-from-scratch-with-pytorch-part-1-4-613dfc1b14c8) de Eric Yang Yu @ UC San Diego.

# Pseudoc칩digo:
![Pseudocodigo](https://github.com/Alo213/PPO/edit/Alo213-patch-1/assets/images/PPO-Clip-Pseudocode.png)
